---
layout: post
category: ml
comments: true
title: "The Least Squares Method"
excerpt: "The least squares method finds the best-fitting line through data by minimizing squared errors. It is the foundation of linear regression and machine learning. Instead of memorizing formulas, we'll in this post discover the solution by working backwards from what we want to achieve. This approach applies far beyond least squares."
author: "Skander Kacem"
tags:
  - Machine Learning
  - Linear Regression
  - Math
katex: true
preview_pic: /assets/0/svd.png
---
The least squares method finds the best fit solution when you have more equations than unknowns. By minimizing the sum of squared errors, we get the closest possible approximation to data that doesn't fit perfectly.

## Begin with the End in Mind: A Mathematical Approach

Stephen Covey wrote about "beginning with the end in mind" as a principle for life planning. The same idea applies to solving mathematical problems: start with what you want to achieve and ask "What conditions would make this true?"

For simple calculus problems, everyone does this automatically. To find the minimum of f(x), solve f'(x) = 0. Easy.

But when problems get complex, where no textbook has your exact setup, we often forget this approach. Imagine designing a communication system to maximize signal-to-noise ratio with constraints on power and bandwidth. No formula exists. So you ask: "What would make SNR maximal? What must be true about my filter coefficients at that maximum?" Work through those conditions, and you've derived your answer.

The backward-working method is powerful when problems get unfamiliar. (I'm grateful to Professor Holger Boche and Dr. Andreas Kortke at Heinrich Heine Institut for teaching me this way of thinking.)

That's exactly what we'll do here: start with what we want (minimize the error) and work backwards to discover the normal equations.

## Why Do We Need This?

Imagine measuring temperature vs. ice cream sales. You have 100 data points but want only 2 parameters: slope and intercept. You have 100 equations but only 2 unknowns. No single line passes through all points perfectly.

Least squares finds the line that gets as close as possible to all points by minimizing the total distance.

## The Problem Setup

Let's formalize this. We have:
- A matrix **A** ∈ ℝ^(m×n) that represents our system
- A measured vector **b** ∈ ℝ^m that contains our observations
- m = number of measurements (equations)
- n = number of parameters we want to estimate (unknowns)
- The key constraint: m > n (more equations than unknowns)

We want to find **x**\* such that:

$$
\| \mathbf{A}\mathbf{x}^* - \mathbf{b} \|_2 = \min_{\mathbf{x}\in\mathbb{R}^n} \| \mathbf{A}\mathbf{x} - \mathbf{b}\|_2 \tag{1}
$$

The vector **r** = **Ax** - **b** is called the **residual**. It represents the errors between our approximation and the actual data. Our goal is to make it as small as possible.

## Deriving the Normal Equations

We want to minimize equation (1). What must be true at the minimum? The gradient must equal zero.

To minimize equation (1), we minimize the squared norm:

$$
f(\mathbf{x}) = \| \mathbf{A}\mathbf{x} - \mathbf{b}\|_2^2 = (\mathbf{A}\mathbf{x} - \mathbf{b})^T(\mathbf{A}\mathbf{x} - \mathbf{b})
$$

Let's expand this:

$$
f(\mathbf{x}) = \mathbf{x}^T\mathbf{A}^T\mathbf{A}\mathbf{x} - 2\mathbf{b}^T\mathbf{A}\mathbf{x} + \mathbf{b}^T\mathbf{b}
$$

To find the minimum, we take the gradient and set it equal to zero:

$$
\nabla f(\mathbf{x}) = 2\mathbf{A}^T\mathbf{A}\mathbf{x} - 2\mathbf{A}^T\mathbf{b} = 0
$$

This gives us the famous **normal equations**:

$$
\mathbf{A}^T\mathbf{A}\mathbf{x} = \mathbf{A}^T\mathbf{b} \tag{2}
$$

We discovered this by asking "what must be true at the minimum?" rather than memorizing a formula. The residual vector **r** = **Ax** - **b** is orthogonal (normal) to the column space of **A** at the optimal solution.

## When Does a Solution Exist?

Good news: the normal equations always have at least one solution. But the nature of that solution depends on the rank of **A**.

**Case 1: Full Rank (rank(A) = n)**

When **A** has full column rank, the matrix **A**^T**A** is:
- Symmetric (obviously, since (**A**^T**A**)^T = **A**^T**A**)
- Positive definite (since **x**^T**A**^T**A**x** = ||**Ax**||^2 > 0 for any **x** ≠ 0)

This means (**A**^T**A**)^(-1) exists and is unique. We get a single, unique solution:

$$
\mathbf{x}^* = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b} \tag{3}
$$

**Case 2: Rank Deficient (rank(A) < min(m,n))**

When **A** doesn't have full rank, **A**^T**A** is singular (not invertible). Multiple solutions exist. In this case, we typically want the solution with the smallest norm:

$$
\mathbf{x}^* = \underset{\mathbf{x} \in X}{\text{argmin}} \|\mathbf{x}\|
$$

where X is the set of all solutions to the normal equations.

This is where the pseudoinverse becomes essential.

## The Pseudoinverse

The term (**A**^T**A**)^(-1)**A**^T from equation (3) appears so often that we give it a special name: the **pseudoinverse** of **A**, written as **A**^†:

$$
\mathbf{A}^\dagger = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T
$$

The pseudoinverse generalizes matrix inversion to rectangular and singular matrices. To be a proper pseudoinverse, **A**^† must satisfy the four Penrose conditions:

1. **A A**^† **A** = **A**
2. **A**^† **A A**^† = **A**^†
3. (**A A**^†)^H = **A A**^†
4. (**A**^† **A**)^H = **A**^† **A**

These conditions ensure **A**^† gives the minimum norm solution when multiple solutions exist.

## Computing the Pseudoinverse via SVD

When **A** is rank deficient, we can't use equation (3) directly because **A**^T**A** isn't invertible. Instead, we use the Singular Value Decomposition (SVD).

Every matrix **A** ∈ ℝ^(m×n) with rank r can be decomposed as:

$$
\mathbf{A} = \mathbf{U} \Sigma \mathbf{V}^T
$$

where:
- **U** ∈ ℝ^(m×m) is orthogonal (columns are orthonormal)
- **V** ∈ ℝ^(n×n) is orthogonal (columns are orthonormal)
- Σ ∈ ℝ^(m×n) is a diagonal matrix with the structure:

$$
\Sigma = \begin{bmatrix}
\Sigma_r & 0 \\
0 & 0
\end{bmatrix}
$$

where Σ_r = diag(σ₁, σ₂, ..., σ_r) and σ₁ ≥ σ₂ ≥ ... ≥ σ_r > 0.

The values σᵢ are called **singular values**. The number of non-zero singular values equals the rank of **A**.

Using SVD, the pseudoinverse is simply:

$$
\mathbf{A}^\dagger = \mathbf{V}\Sigma^\dagger\mathbf{U}^T
$$

where

$$
\Sigma^\dagger = \begin{bmatrix}
\Sigma_r^{-1} & 0 \\
0 & 0
\end{bmatrix}
$$

Notice what happens here: we invert only the non-zero singular values. The zero singular values remain zero. This elegantly handles the rank deficiency.

**Special case:** When rank(**A**) = m = n (square, full rank), then **A**^† = **A**^(-1).

## The Condition Number

Just because we can compute a solution doesn't mean that solution is reliable. Computing **A**^T**A** for the normal equations squares the condition number of **A**, making the problem much more sensitive to numerical errors. SVD avoids this issue.

The condition number measures how sensitive the solution is to small changes in input data:

$$
\kappa(\mathbf{A}) = \frac{\max_{\|\mathbf{x}\| = 1} \|\mathbf{A}\mathbf{x}\|}{\min_{\|\mathbf{x}\| = 1} \|\mathbf{A}\mathbf{x}\|}
$$

For the 2-norm, this simplifies using singular values:

$$
\kappa_2(\mathbf{A}) = \frac{\sigma_{\max}}{\sigma_{\min}} = \frac{\sigma_1}{\sigma_r}
$$

**Practical interpretation:**
- κ(**A**) ≈ 10^k means you lose about k digits of accuracy
- κ(**A**) < 100: well-conditioned
- κ(**A**) > 10^10: ill-conditioned

When a matrix is poorly conditioned, tiny errors can lead to wildly different solutions. Use **truncated SVD** in such cases: ignore the smallest singular values when computing **A**^†.

## Practical Summary

Here's when to use each approach:

**Normal Equations:** Use when **A** is well-conditioned and has full rank. Fast to compute but numerically less stable.

$$
\mathbf{x} = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}
$$

**SVD-based Pseudoinverse:** Use when **A** is rank deficient or poorly conditioned. More expensive computationally but much more stable.

$$
\mathbf{x} = \mathbf{A}^\dagger\mathbf{b} = \mathbf{V}\Sigma^\dagger\mathbf{U}^T\mathbf{b}
$$

**Truncated SVD:** Use when you suspect noise in your measurements. Drop the smallest singular values to avoid amplifying that noise.

The least squares method is fundamental in statistics, machine learning, and engineering. Understanding when and how to apply it correctly makes all the difference.

## References
1. Stephen M. Kogon, Dimitris G. Manolakis, Vinay K. Ingle. "Statistical and Adaptive Signal Processing". Artech House, 2005.
2. [Proofs involving the Moore-Penrose inverse](https://en.wikipedia.org/wiki/Proofs_involving_the_Moore%E2%80%93Penrose_inverse).
3. S. Kacem. "Kompensation von frequenzselektiver I/Q-Imbalance in breitbandigen Direktmisch-Sendern". Studienarbeit 2011.
4. Mathpedia, [Methode der kleinsten Quadrate](https://mathepedia.de/Methode_der_kleinsten_Quadrate.html)
