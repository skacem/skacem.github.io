---
layout: post
category: ml
comments: true
title: "The Least Squares Method"
excerpt: "The least squares method finds the best-fitting line through data by minimizing the sum of squared errors. It's the foundation of linear regression and a cornerstone of machine learning."
author: "Skander Kacem"
tags:
  - Machine Learning
  - Linear Regression
  - Math
katex: true
preview_pic: /assets/0/svd.png
---
The least squares method forms the backbone of linear regression and much of machine learning. At its core, it finds the best-fitting line (or hyperplane) through your data by minimizing the sum of squared errors between predictions and actual values.

Why square the errors? This mathematical choice has elegant properties: it penalizes large errors more heavily than small ones, ensures the solution is unique, and makes the math tractable. You'll see this principle everywhere from simple linear regression to neural network training.

## Problem Setup

Suppose we have a matrix $$\textbf{A} \in \mathbb{K}^{m\text{x}n}$$ representing our features and measurements $$\textbf{b} \in \mathbb{K}^m$$, where $$m$$ is the number of observations, $$n$$ is the number of parameters to estimate, and $$m > n$$ (more data points than parameters).  
We want to find $$\textbf{x}^\ast$$ such that:

$$ 
 \Vert \textbf{A}\textbf{x}^\ast - \textbf{b} \Vert_2 = \underset{x\in\mathbb{K}^n}{\min} \Vert \textbf{A}\textbf{x} - \textbf{b}\Vert_2
$$

Perfect equality $$\Vert \textbf{A}\textbf{x} - \textbf{b}\Vert_2 = 0$$ only happens in ideal cases with exact data. In reality, the vector $$\textbf{b} - \textbf{A}\textbf{x}$$ (called the residual) always exists and represents our prediction errors. Least squares is fundamentally an optimization problem: minimize the Euclidean norm of this residual.

We can derive the normal equation by setting $$\nabla f(x) \stackrel{!}{=} 0$$, where $$f(x) = \Vert \textbf{A}\textbf{x} - \textbf{b}\Vert_2^2$$.  
This transforms our problem: $$\textbf{x}^\ast \in \mathbb{K}^n$$ solves the least squares problem if and only if $$\textbf{x}^\ast$$ solves the normal equations  

$$
\textbf{A}^T\textbf{A}\textbf{x} = \textbf{A}^T\textbf{b}
$$

The normal equation system always has at least one solution.

When $$\textbf{A}$$ has full rank, the solution is unique: $$\textbf{x}^\ast = \textbf{x}$$.  
However, if $$ Rank(\textbf{A}) < \min(m,n)$$, we get a solution set X, where we choose $$\textbf{x}^\ast = \underset{x \in X}{\min}\Vert x \Vert$$ (the minimum norm solution).

Let's explore two key methods for solving the least squares problem. 

## Pseudoinverse and Singular Value Decomposition

When $$\textbf{A}$$ has full rank, $$\textbf{A}^T\textbf{A}$$ becomes symmetric and positive definite. This guarantees that the inverse $$(\textbf{A}^T\textbf{A})^{-1}$$ exists and is unique. We can solve equation (2) directly:

$$
 \textbf{x} = ((\textbf{A}^T\textbf{A})^{-1}\textbf{A}^T)\textbf{b} = \textbf{A}^\dag\textbf{b},

$$

where $$\textbf{A}^\dag = (\textbf{A}^T\textbf{A})^{-1}\textbf{A}^T$$ is called the pseudoinverse.  
The pseudoinverse generalizes matrix inversion to singular and non-square matrices. It must satisfy the Penrose conditions:  

1. $$\textbf{A} \textbf{A}^\dag \textbf{A} = \textbf{A}$$
   * $$\textbf{A}\textbf{A}^\dag$$ doesn't need to be the identity matrix, but it maps all column vectors of A to themselves
2. $$\textbf{A}^\dag \textbf{A} \textbf{A}^\dag = \textbf{A}^\dag$$
   * $$\textbf{A}^\dag$$ behaves like a weak inverse
3. $$(\textbf{A} \textbf{A}^\dag)^H = \textbf{A}\textbf{A}^\dag$$
   * $$\textbf{A}\textbf{A}^\dag$$ is Hermitian
4. $$(\textbf{A}^\dag \textbf{A})^H = \textbf{A}^\dag \textbf{A}$$
   * $$\textbf{A}^\dag \textbf{A}$$ is also Hermitian

When $$Rank(\textbf{A}) < \min(m,n)$$, we define the pseudoinverse using Singular Value Decomposition (SVD).

Let $$\textbf{A} \in \mathbb{K}^{m\text{x}n}$$ be a matrix of rank $$r$$. Then there exist orthogonal matrices $$\textbf{U} \in  \mathbb{K}^{m\text{x}m}$$ and $$\textbf{V}\in\mathbb{K}^{n\text{x}n}$$, and a diagonal matrix 

$$\Sigma = \left[\begin{array}{cc}
 \Sigma_r & 0 \\
   0 & 0
 \end{array}\right]
\in \mathbb{K}^{m\text{x}n}
$$  

with $$\Sigma_r = diag(\sigma_1,\sigma_2,\dots,\sigma_r) \in \mathbb{K}^{r\text{x}r}$$ and $$\sigma_1\geqslant \sigma_2\geqslant \dots \geqslant \sigma_r > 0$$, such that $$\textbf{A}$$ has the decomposition

$$
 \textbf{A} = \textbf{U} \Sigma \textbf{V}^T
$$

This is called the Singular Value Decomposition of $$\textbf{A}$$. The values $$\sigma_i$$ are the singular values of $$\textbf{A}$$. The number of non-zero singular values $$\sigma_i \neq 0$$ equals the rank of $$\textbf{A}$$.

The pseudoinverse of A is then defined as:

$$
 \textbf{A}^\dag = \textbf{V}\Sigma^\dag\textbf{U}^T \in \mathbb{K}^{n\text{x}m}
\text{~, wobei~} \Sigma^\dag = \left[\begin{array}{cc}
\Sigma_r^{-1} & 0 \\
  0 & 0
\end{array}\right]
$$  

When $$Rank(\textbf{A}) = m = n$$, we simply have $$\textbf{A}^\dag = \textbf{A}^{-1}$$.

## Matrix Condition Number

We've shown that matrices can always be "inverted" through approximation, and the normal equation always has at least one solution. But does this solution make sense?

The condition number measures how sensitive our solution is to small changes in the input data. A well-conditioned problem means small input perturbations cause small output changes.  
The condition number of a matrix $$\textbf{A} \in \mathbb{K}^{m\text{x}n}$$ with respect to a norm is defined as

$$
 \kappa_{\Vert \cdot \Vert}(\textbf{A}) = \text{cond}_{\Vert \cdot \Vert}(\textbf{A}) = \frac{\underset{\Vert \textbf{x}\Vert = 1}{\max}  \Vert \textbf{A}\textbf{x}\Vert}{\underset{\Vert \textbf{x}\Vert = 1}{\min} \Vert \textbf{A}\textbf{x}\Vert}
$$

with the following properties:  
* $$\kappa(\textbf{A}) = \kappa(\textbf{A}^{-1})$$
* $$\kappa(\textbf{A}) \geqslant 1$$
* $$\forall \lambda \in \mathbb{C}^\ast \text{: } \kappa(\lambda \textbf{A}) = \kappa(\textbf{A})$$

When dealing with a poorly conditioned matrix, SVD offers a solution: drop the smallest singular values during estimation to avoid noise amplification. This technique, called truncated SVD, trades some bias for significantly reduced variance.

## References
1. Stephen M. Kogon Dimitris G. Manolakis, Vinay K. Ingle. "Statistical and adaptive signal processing". Artech house, 2005. 
2. [Proofs involving the Moore–Penrose inverse](https://en.wikipedia.org/wiki/Proofs_involving_the_Moore%E2%80%93Penrose_inverse).  
3. S. Kacem. ”Kompensation von frequenzselektiver I/Q-Imbalance in breitbandigen Direktmisch-Sendern". Studienarbeit 2011.  
4. Mathpedia, [Methode der kleinsten Quadrate](https://mathepedia.de/Methode_der_kleinsten_Quadrate.html)
